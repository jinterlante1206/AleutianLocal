# podman-compose.override.yml (User creates this file)
version: '0.0.1'

services:
#  orchestrator:
#    environment:
#      OPENAI_API_KEY: ""
#      OPENAI_model: ""
#      OLLAMA_BASE_URL: ""
#      OLLAMA_MODEL: ""
#      SYSTEM_ROLE_PROMPT_PERSONA: "You are an expert systems engineer."
  # Override the LLM command if you've downloaded a different model
  # llm-server:
  #   command: -m /models/my-other-model.gguf -c 8192 --n_gpu_layers -1

#  # Add the custom financial analysis engine
#  finance-engine:
#    build:
#      context: ./path/to/their/finance-engine-code # Or use a pre-built image: image: my-finance-image:latest
#    container_name: user-financial-analysis-engine
#    ports:
#      - "12129:8000" # Expose your service on port 12129
#    networks:
#      - aleutian-network # Connect to the shared network defined in the main compose file

  # Add the InfluxDB instance
  influxdb:
    image: influxdb:latest
    container_name: user-influxdb
    ports:
      - "12130:8086" # Expose your service on port 12130
    volumes:
      - influxdb_data:/var/lib/influxdb2 # Persist InfluxDB data
    networks:
      - aleutian-network

  rag-engine:
    environment:
      RAG_PROMPT_TEMPLATE: "Context: {context}\n\nBased ONLY on the context, answer: {query}\nAnswer:"


volumes:
  influxdb_data: {}
