# Aleutian Local v0.3.0 (Alpha/Dev) - Release Notes

**Date:** November 27, 2025
**Status:** Alpha (Feature-complete for core use cases)
**Audience:** Developers, DevOps Engineers, AI Engineers

## Executive Summary

This release marks a significant architectural pivot from a purely RAG-based retriever to an **Agentic Platform**. Aleutian Local v0.3.0 introduces autonomous agent capabilities (`aleutian trace`) that allow LLMs to actively explore and analyze local codebases, effectively mirroring the functionality of the official Claude CLI but within a secure, private, and policy-enforced perimeter.

We have also deepened integration with **Anthropic's Claude 3.7 Sonnet**, adding support for "Extended Thinking" and "Prompt Caching" to dramatically improve reasoning quality and reduce API costs.

---

## 1. Design Philosophy & Architecture

### **Local-First, Private PaaS**
Aleutian remains committed to the "Local PaaS" philosophy. Data never leaves your local network (or your private API tunnel) without explicit permission. By avoiding third-party "wrapper" tools (like the official `claude` CLI) and instead building our own clients, we ensure that **all file access passes through the Policy Engine**.

### **Bifurcated Intelligence**
We have split the "brain" of the system into two distinct domains:

1.  **Stateless Orchestrator (Go):**
    * **Role:** Router, Policy Enforcer, Auth Gateway, Stateless Chat.
    * **Why Go?** High concurrency for handling Websockets, HTTP routing, and rapid policy scanning without the overhead of a heavy runtime.
    * **Key Change:** Now acts as a proxy for Agent requests, forwarding them to the Python engine while maintaining the security boundary.

2.  **Stateful RAG Engine (Python):**
    * **Role:** Deep Reasoning, Agentic Loops, Tool Execution, Vector Search.
    * **Why Python?** Native ecosystem for AI/ML libraries (Sentence Transformers, Weaviate clients) and complex logic handling.
    * **Key Change:** Moved beyond simple retrieval pipelines to support long-running `while` loops where the LLM "drives" the execution.

### **Direct Tool Use (No MCP)**
We deliberately chose **not** to implement the Model Context Protocol (MCP) at this stage.
* **Reasoning:** MCP adds significant complexity (separate servers, JSON-RPC overhead) and opacity. For a secure local app, we need **explicit control** over what files are read.
* **Approach:** Tools (`list_files`, `read_file`) are hardcoded into the Python `AgentPipeline`. This allows us to inject our `policy_engine.check(path)` logic *directly* before any file `open()` call, guaranteeing security compliance.

---

## 2. New Features & Capabilities

### **A. Agentic Code Tracing (`aleutian trace`)**
* **Command:** `aleutian trace "How does the auth logic work?"`
* **Flow:**
    1.  CLI sends query to Orchestrator.
    2.  Orchestrator proxies to Python RAG Engine.
    3.  **Agent Loop:** Python Service initializes `gemma3:27b` (via Ollama) or `claude-3-7-sonnet`.
    4.  **Reasoning:** The model decides which files to list or read.
    5.  **Execution:** Python executes `ls` or `cat` (after policy checks).
    6.  **Recursion:** The model sees the file content and decides its next move (e.g., "I see `auth.go`, reading it now.").
    7.  **Result:** Returns a final answer plus a step-by-step log of its investigation.
* **Win:** You get "Claude CLI" capability with full privacy control and open-source models.

### **B. Advanced Anthropic Integration**
* **Direct REST Client:** We replaced the `go-anthropic` SDK with a custom REST client in `anthropic_llm.go`.
    * *Why:* The SDK was too strict with types (requiring updates for every new model release). Our client accepts *any* model string (e.g., `claude-3-7-sonnet-20250219`), making it future-proof.
* **Extended Thinking (`--thinking`):**
    * Enables Claude 3.7's hidden chain-of-thought.
    * **Logic:** The client automatically parses the new response format (Block 0: `thinking`, Block 1: `text`) to extract the answer while logging the thoughts.
    * **Budget:** Configurable via `--budget` (default 2048 tokens).
* **Prompt Caching:**
    * The client now structures the `System` prompt as an array of blocks to inject the `cache_control` parameter.
    * *Win:* Reduces costs by ~90% for large RAG contexts.

### **C. CLI Resilience**
* **State Recovery:** Fixed a critical bug where a failed API call (e.g., 400 Bad Request) would leave a "hanging" user message in the local history, causing all subsequent requests to fail validation. The CLI now automatically rolls back the history on error.
* **Dynamic Recompilation:** The architecture relies on `go build` to update the CLI binary, ensuring flags like `--thinking` are immediately available.

---

## 3. Architecture & Data Flow

### **Flow: Direct Chat (Stateless)**
User (CLI) --> [Flags: --thinking] --> Orchestrator (Go) --> Anthropic API (REST) | +--> Weaviate (Session Log)


### **Flow: Agent Trace (Stateful)**
User (CLI) --> Orchestrator (Go) --> [Proxy] --> RAG Engine (Python) | [Agent Loop: Think -> Tool -> Read] | Ollama (Local) OR Anthropic (API) | Local Filesystem (Mounted via Podman Volume)


---

## 4. Interaction Scenarios & Edge Cases

| Scenario | Behavior | Handling |
| :--- | :--- | :--- |
| **Using `--thinking` with incompatible model (e.g., Claude 3.5)** | **Error 404/400.** Anthropic rejects the `thinking` param. | **Fix:** Orchestrator returns the error JSON. CLI prints it. User must switch `CLAUDE_MODEL` in config. |
| **Agent Loop hits Max Steps** | Loop terminates after 10-15 iterations. | **Fix:** Returns "Trace limit reached" + partial logs so work isn't lost. |
| **Empty Assistant Message** | Occurs if "Thinking" block is present but parsing logic fails to find "Text" block. | **Fix:** `anthropic_llm.go` updated to iterate through *all* blocks and concatenate text parts, ignoring `thinking` blocks for the final string. |
| **Podman Volume Missing** | Container crashes immediately with `statfs error`. | **Fix:** Requires `podman machine rm` and `init -v /Volumes:/Volumes` to bridge the host filesystem. |

---

## 5. Current Problem Status (ASCII Table)

| Problem | Severity | Current Workaround | Ideal Solution | Priority |
| :--- | :---: | :--- | :--- | :---: |
| **Hardcoded Paths** | **Critical** | Manually editing `override.yml` to remove `/Users/jin/...`. | Use relative paths (`./services/...`) in all Compose files. | High |
| **Secret Leakage** | **Critical** | Hardcoded `INFLUXDB_TOKEN` in `override.yml`. | Use `${ENV_VAR}` substitution and `.env` file. | High |
| **Podman Mounts** | High | Manual `podman machine init` commands required. | Script the machine setup (`setup_mac.sh`) to auto-detect volumes. | Med |
| **Agent Latency** | Med | User waits for full trace to finish (no feedback). | Implement WebSocket streaming for Agent steps (show "Reading file..." in real-time). | Med |
| **Model Switching** | Low | Requires restart to change `LLM_BACKEND_TYPE`. | Add runtime config endpoint to swap backends dynamically. | Low |

---

## 6. Technical Debt & Known Issues

1.  **Parsing Logic duplication:** We have "Tool Parsing" logic in Python (for Agents) and "Response Parsing" logic in Go (for Chat). Ideally, we'd standardize on one schema, but the language barrier makes this acceptable for now.
2.  **Configuration Sprawl:** Config is split between `config.yaml`, `podman-compose.yml`, and `podman-compose.override.yml`. We need to consolidate "User Config" into a single source of truth that the startup script parses.
3.  **Missing "Thinking" in Python:** The `aleutian ask` (RAG) command currently reads `ENABLE_THINKING` from an env var, not the CLI flags. We need to wire the CLI flags all the way through to the Python service payload.

---

## 7. The Wins

* **Weaviate Integration:** Full schema management and session logging is working seamlessly.
* **Policy Enforcement:** Unlike generic tools, **Aleutian scans every file** before the LLM sees it. This is a massive compliance win.
* **"Thinking" Support:** We successfully reverse-engineered the Claude 3.7 API requirements (separate blocks, max_token adjustment) to enable deep reasoning.
* **Cost Savings:** Prompt Caching is active, potentially saving hundreds of dollars on heavy RAG queries.
* **Portable Stack:** Despite the Podman hurdles, the entire stack (DB, Metrics, Apps) spins up with a single command (`aleutian stack start`) on a fresh machine (assuming volume mounts are correct).

---

### Next Steps for Production Push
1.  **Sanitize:** Remove all absolute paths (`/Users/jin`) and hardcoded tokens from `podman-compose.override.yml`.
2.  **Rebuild:** Run `aleutian stack start --build` one last time to confirm Dockerfiles build with relative paths.
3.  **Commit:** Push the clean code.