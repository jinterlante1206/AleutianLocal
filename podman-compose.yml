secrets:
  aleutian_hf_token:
    external: true
  openai_api_key:
    external: true

services:
  # The Embedding server (Built from source in this repo)
  embedding-server:
    build:
      context: ./services/embeddings # Points to the embeddings source code included in this repo
    container_name: aleutian-embedding-server
    ports: ["${EMBEDDING_PORT:-12126}:8000"] # Assuming your embeddings server runs on 8000 internally
    environment:
      MODEL_NAME: ${EMBEDDING_MODEL_ID:-google/embeddinggemma-300m} # Defaults to Gemma embeddings
    secrets:
      - source: aleutian_hf_token
    volumes:
      - ./models_cache:/root/.cache/huggingface/
    networks:
      - aleutian-network

  # The Orchestrator (Built from source in this repo)
  orchestrator:
    build:
      context: .
      dockerfile: ./services/orchestrator/Dockerfile
    container_name: aleutian-go-orchestrator
    ports: ["${ORCHESTRATOR_PORT:-12210}:12210"] # Use configured port
    environment:
      PORT: ${ORCHESTRATOR_PORT:-12210}
      EMBEDDING_SERVICE_URL: http://embedding-server:8000/embed # Points to the internal port
      LLM_BACKEND_TYPE: "ollama" # this governs the switch statement in main.go
      OLLAMA_BASE_URL: "http://host.containers.internal:11434" # Point to host Ollama
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gpt-oss}
      OPENAI_URL_BASE: "https://api.openai.com/v1"
      RAG_ENGINE_URL: http://aleutian-rag-engine:8000
      WEAVIATE_SERVICE_URL: http://weaviate-db:8080
      POLICY_ENGINE_DATA_CLASSIFICATION_PATTERNS_PATH: /app/internal/policy_engine/enforcement/data_classification_patterns.yaml
      ALEUTIAN_CUSTOM_TOOL_FINANCE: ${ALEUTIAN_CUSTOM_TOOL_FINANCE:-}
      ALEUTIAN_CUSTOM_TOOL_INFLUX: ${ALEUTIAN_CUSTOM_TOOL_INFLUX:-}
    volumes:
      # Mount the policy file so users can customize it locally
      - ./internal/policy_engine/enforcement/data_classification_patterns.yaml:/app/internal/policy_engine/enforcement/data_classification_patterns.yaml:ro
    secrets:
      - source: aleutian_hf_token
    networks:
      - aleutian-network
    depends_on:
      - embedding-server
      - weaviate-db
      - aleutian-rag-engine

  # The Vector Database
  weaviate-db:
    image: cr.weaviate.io/semitechnologies/weaviate:latest
    container_name: aleutian-weaviate-db
    ports: ["${WEAVIATE_PORT:-12127}:8080", "50051:50051"]
    volumes:
      - weaviate_data:/var/lib/weaviate # Use a named volume for data persistence
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      CLUSTER_HOSTNAME: 'weaviate-db'
    networks:
      - aleutian-network

  gguf-converter:
    build:
      context: .
      dockerfile: ./services/gguf_converter/Dockerfile
    container_name: aleutian-gguf-converter
    ports:
      - "12140:8000"
    volumes:
      - ./models:/models
      - ./models_cache:/root/.cache/huggingface/
    secrets:
      - source: aleutian_hf_token
    networks:
      - aleutian-network

  aleutian-rag-engine:
    build:
      context: ./services/rag_engine
      dockerfile: Dockerfile
    container_name: aleutian-rag-engine
    ports:
      - "${RAG_ENGINE_PORT:-12125}:8000" # Expose internal port 8000 on host
    environment:
      # --- Connection Info ---
      WEAVIATE_SERVICE_URL: ${WEAVIATE_SERVICE_URL:-http://weaviate-db:8080}
      EMBEDDING_SERVICE_URL: ${EMBEDDING_SERVICE_URL:-http://embedding-server:8000/embed}
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317} # For OTel

      # --- LLM Backend Configuration (Mirrors orchestrator) ---
      LLM_BACKEND_TYPE: ${LLM_BACKEND_TYPE:-ollama}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-[http://host.containers.internal:11434](http://host.containers.internal:11434)}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3}
      LLM_SERVICE_URL_BASE: ${LLM_SERVICE_URL_BASE:-http://llm-server:8080}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      OPENAI_URL_BASE: ${OPENAI_URL_BASE:-[https://api.openai.com/v1](https://api.openai.com/v1)}
      HF_SERVER_URL: ${HF_SERVER_URL:-http://hf-server:8000} # Add if using hf-transformers
      # Add others (CLAUDE_MODEL etc.) if implementing those clients

      # --- Pipeline Configuration ---
      LLM_DEFAULT_TEMPERATURE: ${LLM_DEFAULT_TEMPERATURE:-0.5}
      LLM_DEFAULT_MAX_TOKENS: ${LLM_DEFAULT_MAX_TOKENS:-1024}
      LLM_DEFAULT_TOP_K: ${LLM_DEFAULT_TOP_K:-40}
      LLM_DEFAULT_TOP_P: ${LLM_DEFAULT_TOP_P:-0.9}
      LLM_DEFAULT_STOP_SEQUENCES: ${LLM_DEFAULT_STOP_SEQUENCES:-'["\\n"]'} # JSON list as string
      RAG_PROMPT_TEMPLATE: ${RAG_PROMPT_TEMPLATE:-"You are a helpful assistant..."} # Put default here or keep in code
      RERANKER_MODEL: ${RERANKER_MODEL:-"cross-encoder/ms-marco-MiniLM-L-6-v2"}
      RERANK_INITIAL_K: ${RERANK_INITIAL_K:-20}
      RERANK_FINAL_K: ${RERANK_FINAL_K:-5}

    secrets:
      # Mount secrets needed by the pipelines (e.g., API keys)
      - source: openai_api_key # Read by StandardRAGPipeline._read_secret
      # Add other secrets (anthropic_api_key, etc.) if needed
      # - source: anthropic_api_key
    networks:
      - aleutian-network
    depends_on:
      - embedding-server
#      - weaviate-db



volumes:
  weaviate_data:
  models_cache: {}

networks:
  aleutian-network:
    driver: bridge
    name: aleutian-network # Explicitly name the network for easier overrides