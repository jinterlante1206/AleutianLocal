secrets:
  aleutian_hf_token:
    external: true
  openai_api_key:
    external: true

services:
  # The Embedding server (Built from source in this repo)
  embedding-server:
    build:
      context: ./services/embeddings # Points to the embeddings source code included in this repo
    container_name: aleutian-embedding-server
    ports: ["${EMBEDDING_PORT:-12126}:8000"]
    environment:
      MODEL_NAME: ${EMBEDDING_MODEL_ID:-google/embeddinggemma-300m} # Defaults to Gemma embeddings
    secrets:
      - source: aleutian_hf_token
    volumes:
      - ./models_cache:/root/.cache/huggingface/
    networks:
      - aleutian-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 10s
      timeout: 5s
      retries: 20

  # The Orchestrator (Built from source in this repo)
  orchestrator:
    build:
      context: .
      dockerfile: ./services/orchestrator/Dockerfile
    container_name: aleutian-go-orchestrator
    ports: ["${ORCHESTRATOR_PORT:-12210}:12210"]
    environment:
      # --- Core Config ---
      PORT: ${ORCHESTRATOR_PORT:-12210}
      EMBEDDING_SERVICE_URL: ${EMBEDDING_SERVICE_URL:-http://embedding-server:8000/embed}
      RAG_ENGINE_URL: ${RAG_ENGINE_URL:-http://aleutian-rag-engine:8000}
      WEAVIATE_SERVICE_URL: ${WEAVIATE_SERVICE_URL:-http://weaviate-db:8080}
      POLICY_ENGINE_DATA_CLASSIFICATION_PATTERNS_PATH: ${POLICY_ENGINE_DATA_CLASSIFICATION_PATTERNS_PATH:-/app/internal/policy_engine/enforcement/data_classification_patterns.yaml}
      OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-otel-collector:4317}

      # --- LLM Backend Configuration ---
      LLM_BACKEND_TYPE: ${LLM_BACKEND_TYPE:-ollama}
      # Ollama Specific
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.containers.internal:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gpt-oss:latest}
      # OpenAI Specific
      OPENAI_URL_BASE: ${OPENAI_URL_BASE:-https://api.openai.com/v1}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini} # Note: Key comes from secret
      # Local Llama.cpp Specific
      LLM_SERVICE_URL_BASE: ${LLM_SERVICE_URL_BASE:-} # Base URL for llama.cpp server
      # HF TGI/vLLM Specific
      HF_SERVER_URL: ${HF_SERVER_URL:-}
      # Generic Remote Specific
      REMOTE_LLM_URL: ${REMOTE_LLM_URL:-}
      # Add Anthropic/Gemini base URLs if needed later

      # --- FUTURE: Data Parsing Service URLs ---
      PDF_PARSER_URL: ${PDF_PARSER_URL:-}
      DOCX_PARSER_URL: ${DOCX_PARSER_URL:-}
      HTML_CLEANER_URL: ${HTML_CLEANER_URL:-}
      AUDIO_TRANSCRIBER_URL: ${AUDIO_TRANSCRIBER_URL:-}
      OCR_SERVICE_URL: ${OCR_SERVICE_URL:-}
      PII_MASKING_SERVICE_URL: ${PII_MASKING_SERVICE_URL:-}

      # --- FUTURE: Agent Tools & Services ---
      TOOL_REGISTRY_URL: ${TOOL_REGISTRY_URL:-}
      AGENT_STATE_DB_CONNECTION_STRING: ${AGENT_STATE_DB_CONNECTION_STRING:-}
      # Examples of custom tool URLs (already present from template)
      ALEUTIAN_CUSTOM_TOOL_FINANCE: ${ALEUTIAN_CUSTOM_TOOL_FINANCE:-}
      ALEUTIAN_CUSTOM_TOOL_INFLUX: ${ALEUTIAN_CUSTOM_TOOL_INFLUX:-}
      # Add more generic custom tool slots if desired:
      CUSTOM_TOOL_1_URL: ${CUSTOM_TOOL_1_URL:-}
      CUSTOM_TOOL_2_URL: ${CUSTOM_TOOL_2_URL:-}

      # --- FUTURE: Evaluation Framework ---
      EVALUATION_ENGINE_URL: ${EVALUATION_ENGINE_URL:-}

      # --- FUTURE: General Config ---
      LOG_LEVEL: ${LOG_LEVEL:-INFO} # Example for controlling orchestrator logs
      TIMEOUT_SECONDS_RAG_ENGINE: ${TIMEOUT_SECONDS_RAG_ENGINE:-240} # Example timeout
      TIMEOUT_SECONDS_LLM: ${TIMEOUT_SECONDS_LLM:-300} # Example timeout
    volumes:
      # Mount the policy file so users can customize it locally
      - ./internal/policy_engine/enforcement/data_classification_patterns.yaml:/app/internal/policy_engine/enforcement/data_classification_patterns.yaml:ro
    secrets:
      - source: aleutian_hf_token
      - source: openai_api_key
    networks:
      - aleutian-network
    restart: unless-stopped
    depends_on:
      aleutian-rag-engine:
        condition: service_started
      embedding-server:
        condition: service_healthy
      weaviate-db:
        condition: service_healthy
      otel-collector:
        condition: service_started
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:12210/health" ]
      interval: 10s
      timeout: 5s
      retries: 20

  # The Vector Database
  weaviate-db:
    image: cr.weaviate.io/semitechnologies/weaviate:latest
    container_name: aleutian-weaviate-db
    ports: ["${WEAVIATE_PORT:-12127}:8080", "50051:50051"]
    volumes:
      - weaviate_data:/var/lib/weaviate # Use a named volume for data persistence
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      CLUSTER_HOSTNAME: 'weaviate-db'
    restart: unless-stopped
    networks:
      - aleutian-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/v1/.well-known/ready"]
      interval: 10s
      timeout: 5s
      retries: 20

  gguf-converter:
    build:
      context: .
      dockerfile: ./services/gguf_converter/Dockerfile
    container_name: aleutian-gguf-converter
    ports:
      - "12140:8000"
    volumes:
      - ./models:/models
      - ./models_cache:/root/.cache/huggingface/
    secrets:
      - source: aleutian_hf_token
    restart: unless-stopped
    networks:
      - aleutian-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 10s
      timeout: 5s
      retries: 20

  aleutian-rag-engine:
    build:
      context: ./services/rag_engine
      dockerfile: Dockerfile
    container_name: aleutian-rag-engine
    restart: unless-stopped
    ports:
      - "${RAG_ENGINE_PORT:-12125}:8000" # Expose internal port 8000 on host
    environment:
      # --- Connection Info ---
      WEAVIATE_SERVICE_URL: ${WEAVIATE_SERVICE_URL:-http://weaviate-db:8080}
      EMBEDDING_SERVICE_URL: ${EMBEDDING_SERVICE_URL:-http://embedding-server:8000/embed}
      OTEL_EXPORTER_OTLP_ENDPOINT: otel-collector:4317
      LLM_BACKEND_TYPE: ${LLM_BACKEND_TYPE:-ollama}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.containers.internal:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gpt-oss}
      LLM_SERVICE_URL_BASE: ${LLM_SERVICE_URL_BASE:-http://llm-server:8080}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      OPENAI_URL_BASE: ${OPENAI_URL_BASE:-https://api.openai.com/v1}
      HF_SERVER_URL: ${HF_SERVER_URL:-http://hf-server:8000}
      # Add others (CLAUDE_MODEL etc.) if implementing those clients
      # --- Pipeline Configuration ---
      LLM_DEFAULT_TEMPERATURE: ${LLM_DEFAULT_TEMPERATURE:-0.5}
      LLM_DEFAULT_MAX_TOKENS: ${LLM_DEFAULT_MAX_TOKENS:-1024}
      LLM_DEFAULT_TOP_K: ${LLM_DEFAULT_TOP_K:-40}
      LLM_DEFAULT_TOP_P: ${LLM_DEFAULT_TOP_P:-0.9}
      LLM_DEFAULT_STOP_SEQUENCES: '["\\n"]'
      RERANKER_MODEL: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      RERANK_INITIAL_K: ${RERANK_INITIAL_K:-20}
      RERANK_FINAL_K: ${RERANK_FINAL_K:-5}
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 10s
      timeout: 5s
      retries: 20
    secrets:
      # Mount secrets needed by the pipelines (e.g., API keys)
      - source: openai_api_key # Read by StandardRAGPipeline._read_secret
      # Add other secrets (anthropic_api_key, etc.) if needed
      # - source: anthropic_api_key
    networks:
      - aleutian-network
    depends_on:
      embedding-server:
        condition: service_healthy
      weaviate-db:
        condition: service_healthy

  # --- Observability Stack ---
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: aleutian-otel-collector
    command: ["--config=/etc/otelcol-contrib/config.yaml"]
    volumes:
      - ./observability/otel_collector_config.yaml:/etc/otelcol-contrib/config.yaml:ro
    ports:
      - "4317:4317"  # OTLP gRPC (used by your apps)
      - "4318:4318"  # OTLP HTTP
      - "8889:8889"  # Prometheus exporter (for prometheus to scrape)
      - "14250:14250" # Jaeger gRPC exporter (for jaeger to receive)
      - "13133:13133"
    networks:
      - aleutian-network
    depends_on:
      - aleutian-jaeger

  aleutian-jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: aleutian-jaeger
    ports:
      - "16686:16686"
    networks:
      - aleutian-network
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:16686/"]
      interval: 10s
      timeout: 5s
      retries: 5

  prometheus:
    image: prom/prometheus:latest
    container_name: aleutian-prometheus
    volumes:
      - ./observability/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro
    ports:
      - "9090:9090"
    networks:
      - aleutian-network
    depends_on:
      - otel-collector
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:latest
    container_name: aleutian-grafana
    ports:
      - "3000:3000"
    volumes:
      - ./observability/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana # persistent data volume for dashboards, etc.
    networks:
      - aleutian-network
    depends_on:
      - prometheus
      - aleutian-jaeger
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  weaviate_data:
  models_cache: {}
  grafana_data: {}

networks:
  aleutian-network:
    driver: bridge
    name: aleutian-network # Explicitly name the network for easier overrides