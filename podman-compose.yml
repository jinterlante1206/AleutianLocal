# Aleutian Local - Base RAG Configuration
# Simplified compose for knowledge base / RAG use case with Ollama
#
# Prerequisites:
#   - Ollama running on host (for LLM and embeddings)
#   - Podman secrets configured: podman secret create aleutian_hf_token ~/.hf_token
#
# Usage:
#   aleutian stack start                    # Start all services (auto-pulls models)
#   aleutian ingest ./docs                  # Ingest documents
#   aleutian chat                           # Start chatting
#
# Ports:
#   12210 - Orchestrator API
#   12125 - RAG Engine
#   12127 - Weaviate
#   16686 - Jaeger UI (tracing)

secrets:
  aleutian_hf_token:
    external: true

services:
  # === Vector Database ===
  # Stores document embeddings for semantic search
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:latest
    container_name: aleutian-weaviate
    ports:
      - "12127:8080"
      - "50051:50051"
    volumes:
      - weaviate_data:/var/lib/weaviate
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      DEFAULT_VECTORIZER_MODULE: "none"
      CLUSTER_HOSTNAME: "weaviate"
    networks:
      - aleutian-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/v1/.well-known/ready"]
      interval: 10s
      timeout: 5s
      retries: 10

  # === RAG Engine ===
  # Python service that handles retrieval and generation pipelines
  rag-engine:
    build:
      context: ./services/rag_engine
      dockerfile: Dockerfile
    container_name: aleutian-rag-engine
    ports:
      - "12125:8000"
    environment:
      # Connections
      WEAVIATE_SERVICE_URL: http://weaviate:8080
      EMBEDDING_SERVICE_URL: http://host.containers.internal:11434/api/embed
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-nomic-embed-text-v2-moe}
      OTEL_EXPORTER_OTLP_ENDPOINT: jaeger:4317
      # LLM Config - Ollama on host
      LLM_BACKEND_TYPE: ollama
      OLLAMA_BASE_URL: http://host.containers.internal:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gpt-oss}
      # Pipeline Config
      LLM_DEFAULT_TEMPERATURE: 0.7
      LLM_DEFAULT_MAX_TOKENS: 2048
      RERANKER_MODEL: cross-encoder/ms-marco-MiniLM-L-6-v2
      RERANK_INITIAL_K: 20
      RERANK_FINAL_K: 5
      # Verified Pipeline Role Temperatures (uncomment to override defaults)
      # Lower = more deterministic, Higher = more creative
      # All default to 0.6 for model compatibility (some models fail below 0.5)
      # VERIFIED_OPTIMIST_TEMPERATURE: 0.6  # Draft generation (creative)
      # VERIFIED_SKEPTIC_TEMPERATURE: 0.6   # Verification audit (deterministic)
      # VERIFIED_REFINER_TEMPERATURE: 0.6   # Hallucination correction (balanced)
    volumes:
      - models_cache:/root/.cache/huggingface
    networks:
      - aleutian-network
    restart: unless-stopped
    depends_on:
      weaviate:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 20

  # === Orchestrator ===
  # Go service that coordinates all components and exposes the API
  orchestrator:
    build:
      context: .
      dockerfile: ./services/orchestrator/Dockerfile
    container_name: aleutian-go-orchestrator
    ports:
      - "12210:12210"
    environment:
      PORT: 12210
      # Service URLs
      WEAVIATE_SERVICE_URL: http://weaviate:8080
      RAG_ENGINE_URL: http://rag-engine:8000
      EMBEDDING_SERVICE_URL: http://host.containers.internal:11434/api/embed
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-nomic-embed-text-v2-moe}
      # LLM Config - Ollama on host
      LLM_BACKEND_TYPE: ollama
      OLLAMA_BASE_URL: http://host.containers.internal:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gpt-oss}
      # Observability
      OTEL_EXPORTER_OTLP_ENDPOINT: jaeger:4317
    volumes:
      - ./services/policy_engine/data_classification_patterns.yaml:/app/data_classification_patterns.yaml:ro
    networks:
      - aleutian-network
    restart: unless-stopped
    depends_on:
      rag-engine:
        condition: service_healthy
      weaviate:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12210/health"]
      interval: 10s
      timeout: 5s
      retries: 20

  # === Observability (Minimal) ===
  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: aleutian-jaeger
    ports:
      - "16686:16686"   # UI
      - "4317:4317"     # OTLP gRPC
      - "4318:4318"     # OTLP HTTP
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    networks:
      - aleutian-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:16686/"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  weaviate_data:
  models_cache:

networks:
  aleutian-network:
    driver: bridge
    name: aleutian-network
