// Copyright (C) 2025 Aleutian AI (jinterlante@aleutian.ai)
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
// See the LICENSE.txt file for the full license text.
//
// NOTE: This work is subject to additional terms under AGPL v3 Section 7.
// See the NOTICE.txt file for details regarding AI system attribution.

package handlers

import (
	"context"
	"errors"
	"log/slog"
	"net/http"
	"sync/atomic"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/jinterlante1206/AleutianLocal/services/llm"
	"github.com/jinterlante1206/AleutianLocal/services/orchestrator/datatypes"
	"github.com/jinterlante1206/AleutianLocal/services/orchestrator/observability"
	"github.com/jinterlante1206/AleutianLocal/services/orchestrator/services"
	"github.com/jinterlante1206/AleutianLocal/services/policy_engine"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/codes"
	"go.opentelemetry.io/otel/trace"
)

// =============================================================================
// Constants
// =============================================================================

const (
	// heartbeatInterval is the interval for sending keepalive pings.
	// Set to 15s to stay well under typical LB timeouts (60s for ALB/Nginx).
	heartbeatInterval = 15 * time.Second
)

// =============================================================================
// Streaming Callback Types
// =============================================================================

// StreamCallback is called for each token or event during streaming.
//
// # Description
//
// StreamCallback receives tokens as they are generated by the LLM.
// The callback should write each token to the SSE stream.
// Return an error to abort streaming (e.g., on client disconnect).
//
// # Inputs
//
//   - event: Stream event containing token content, thinking, or error.
//
// # Outputs
//
//   - error: Non-nil to abort streaming.
//
// # Examples
//
//	callback := func(event llm.StreamEvent) error {
//	    return sseWriter.WriteToken(event.Content)
//	}
//
// # Limitations
//
//   - Must be safe to call from any goroutine.
//
// # Assumptions
//
//   - Called in token order.
type StreamCallback = llm.StreamCallback

// =============================================================================
// Interface Definition
// =============================================================================

// StreamingChatHandler defines the contract for handling streaming chat HTTP requests.
//
// # Description
//
// StreamingChatHandler abstracts streaming chat endpoint handling, enabling different
// implementations and facilitating testing via mocks. The interface provides
// Server-Sent Events (SSE) streaming endpoints.
//
// # Security Model
//
// - Outbound (user → system): Blocked if contains sensitive data (policy engine)
// - Inbound (system → user): Allowed, logged for audit via hash chain
// - RAG context: Logged if contains PII (future: granite-guardian integration)
//
// # Thread Safety
//
// Implementations must be safe for concurrent use by multiple goroutines.
// HTTP handlers are called concurrently by the Gin framework.
//
// # Limitations
//
//   - Requires LLM client that supports streaming (ChatStream method)
//   - Client must support SSE (EventSource or similar)
//
// # Assumptions
//
//   - All dependencies are properly initialized before handler use
//   - Gin context is valid and not nil
//   - LLM client implements ChatStream method
type StreamingChatHandler interface {
	// HandleDirectChatStream processes direct LLM chat requests with SSE streaming.
	//
	// # Description
	//
	// Handles POST /v1/chat/direct/stream requests. Streams tokens as they
	// are generated by the LLM via Server-Sent Events.
	//
	// # Inputs
	//
	//   - c: Gin context containing the HTTP request.
	//
	// # Outputs
	//
	// SSE stream with events:
	//   - status: Processing status updates
	//   - token: Generated tokens
	//   - thinking: Extended thinking content (if enabled)
	//   - done: Stream completion with session ID
	//   - error: Error events (if failure occurs)
	//
	// # Limitations
	//
	//   - No sources event (direct chat has no RAG)
	//
	// # Assumptions
	//
	//   - Client supports SSE
	HandleDirectChatStream(c *gin.Context)

	// HandleChatRAGStream processes conversational RAG requests with SSE streaming.
	//
	// # Description
	//
	// Handles POST /v1/chat/rag/stream requests. Retrieves context from
	// vector database, then streams LLM response via Server-Sent Events.
	//
	// # Inputs
	//
	//   - c: Gin context containing the HTTP request.
	//
	// # Outputs
	//
	// SSE stream with events:
	//   - status: Processing status updates
	//   - sources: Retrieved documents with scores
	//   - token: Generated tokens
	//   - done: Stream completion with session ID
	//   - error: Error events (if failure occurs)
	//
	// # Limitations
	//
	//   - Requires Weaviate client for RAG retrieval
	//
	// # Assumptions
	//
	//   - Client supports SSE
	//   - RAG service is available
	HandleChatRAGStream(c *gin.Context)
}

// =============================================================================
// Struct Definition
// =============================================================================

// streamingChatHandler implements StreamingChatHandler for production use.
//
// # Description
//
// streamingChatHandler coordinates between HTTP layer and streaming business logic.
// It performs HTTP-related tasks and delegates LLM streaming to injected services:
//   - Request parsing and validation
//   - SSE header configuration
//   - Stream event emission
//   - Error handling and cleanup
//
// # Fields
//
//   - llmClient: LLM client with streaming support (must implement ChatStream)
//   - policyEngine: Policy engine for sensitive data scanning
//   - ragService: Service for RAG chat processing (may be nil)
//   - tracer: OpenTelemetry tracer for distributed tracing
//
// # Thread Safety
//
// Thread-safe. All fields are read-only after construction.
// No shared mutable state between requests.
//
// # Limitations
//
//   - Requires LLM client that supports ChatStream method
//   - RAG streaming requires ragService to be non-nil
//
// # Assumptions
//
//   - Dependencies are non-nil and properly configured
//   - LLM client supports streaming
type streamingChatHandler struct {
	llmClient    llm.LLMClient
	policyEngine *policy_engine.PolicyEngine
	ragService   *services.ChatRAGService
	tracer       trace.Tracer
}

// =============================================================================
// Constructor
// =============================================================================

// NewStreamingChatHandler creates a StreamingChatHandler with the provided dependencies.
//
// # Description
//
// Creates a fully configured streamingChatHandler for production use.
// All dependencies must be properly initialized before calling.
// Panics if llmClient or policyEngine is nil (programming errors).
//
// # Inputs
//
//   - llmClient: LLM client with streaming support. Must not be nil.
//     Must implement ChatStream method for streaming to work.
//   - policyEngine: Policy scanner. Must not be nil.
//   - ragService: RAG chat service. May be nil if RAG is not used.
//
// # Outputs
//
//   - StreamingChatHandler: Ready for use with Gin router
//
// # Examples
//
//	handler := handlers.NewStreamingChatHandler(llmClient, policyEngine, ragService)
//	router.POST("/v1/chat/direct/stream", handler.HandleDirectChatStream)
//	router.POST("/v1/chat/rag/stream", handler.HandleChatRAGStream)
//
// # Limitations
//
//   - Panics on nil llmClient or policyEngine
//
// # Assumptions
//
//   - llmClient and policyEngine are non-nil and ready for use
//   - llmClient supports ChatStream method
func NewStreamingChatHandler(
	llmClient llm.LLMClient,
	policyEngine *policy_engine.PolicyEngine,
	ragService *services.ChatRAGService,
) StreamingChatHandler {
	if llmClient == nil {
		panic("NewStreamingChatHandler: llmClient must not be nil")
	}
	if policyEngine == nil {
		panic("NewStreamingChatHandler: policyEngine must not be nil")
	}

	return &streamingChatHandler{
		llmClient:    llmClient,
		policyEngine: policyEngine,
		ragService:   ragService,
		tracer:       otel.Tracer("aleutian.orchestrator.handlers.chat_streaming"),
	}
}

// =============================================================================
// Handler Methods
// =============================================================================

// HandleDirectChatStream processes direct LLM chat requests with SSE streaming.
//
// # Description
//
// Handles POST /v1/chat/direct/stream requests. The flow is:
//  1. Parse and validate request body
//  2. Scan last user message for policy violations (outbound protection)
//  3. Set SSE headers and create writer
//  4. Emit status event
//  5. Stream tokens from LLM via ChatStream
//  6. Emit done event with session info
//
// # Security
//
// - Outbound (user → LLM): Scanned and blocked if contains sensitive data
// - Inbound (LLM → user): Allowed, logged via hash chain for async audit
//
// # Inputs
//
//   - c: Gin context containing the HTTP request
//
// Request Body (datatypes.DirectChatRequest):
//   - request_id: Required. UUID v4 identifier for tracing.
//   - timestamp: Required. Unix timestamp in milliseconds (UTC).
//   - messages: Required. Array of message objects (1-100) with role and content.
//   - enable_thinking: Optional. Enable extended thinking mode.
//   - budget_tokens: Optional. Token budget for thinking (0-65536).
//
// # Outputs
//
// SSE Events:
//   - event: status, data: {"type":"status","message":"Generating response..."}
//   - event: token, data: {"type":"token","content":"Hello"}
//   - event: thinking, data: {"type":"thinking","content":"Let me think..."}
//   - event: done, data: {"type":"done","session_id":"..."}
//   - event: error, data: {"type":"error","error":"..."}
//
// HTTP Status (before streaming starts):
//   - 400 Bad Request: Invalid request body or validation failure
//   - 403 Forbidden: Policy violation detected (sensitive data in outbound message)
//   - 500 Internal Server Error: SSE setup failure
//
// # Examples
//
// Request:
//
//	POST /v1/chat/direct/stream
//	Accept: text/event-stream
//	{
//	    "request_id": "550e8400-e29b-41d4-a716-446655440000",
//	    "timestamp": 1735817400000,
//	    "messages": [{"role": "user", "content": "Hello"}]
//	}
//
// Response (SSE stream):
//
//	event: status
//	data: {"type":"status","message":"Generating response...","id":"...","created_at":...}
//
//	event: token
//	data: {"type":"token","content":"Hello","id":"...","created_at":...}
//
//	event: done
//	data: {"type":"done","session_id":"...","id":"...","created_at":...}
//
// # Limitations
//
//   - Only scans last user message for policy (not full history)
//   - Errors during streaming are sent as events, not HTTP errors
//
// # Assumptions
//
//   - Request body is valid JSON
//   - LLM client supports ChatStream method
//   - Client supports SSE
//
// # Security References
//
//   - SEC-003: Message size limits enforced via validation
//   - SEC-005: Internal errors not exposed to client
func (h *streamingChatHandler) HandleDirectChatStream(c *gin.Context) {
	startTime := time.Now()
	endpoint := observability.EndpointDirectStream

	ctx, span := h.tracer.Start(c.Request.Context(), "HandleDirectChatStream")
	defer span.End()

	// Track active stream (for metrics)
	if m := observability.DefaultMetrics; m != nil {
		m.StreamStarted(endpoint)
		defer m.StreamEnded(endpoint)
	}

	success := false
	defer func() {
		// Record final metrics
		if m := observability.DefaultMetrics; m != nil {
			duration := time.Since(startTime).Seconds()
			m.RecordRequest(endpoint, success)
			m.RecordStreamDuration(endpoint, duration, success)
		}
	}()

	// Step 1: Parse request body
	var req datatypes.DirectChatRequest
	if err := c.BindJSON(&req); err != nil {
		span.RecordError(err)
		span.SetStatus(codes.Error, "invalid request body")
		slog.Error("Failed to parse streaming chat request", "error", err)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeValidation)
		}
		c.JSON(http.StatusBadRequest, gin.H{"error": "invalid request body"})
		return
	}

	// Add request attributes to span
	span.SetAttributes(
		attribute.String("request.id", req.RequestID),
		attribute.Int("request.message_count", len(req.Messages)),
		attribute.Bool("request.thinking_enabled", req.EnableThinking),
	)

	// Step 2: Validate request
	if err := req.Validate(); err != nil {
		span.RecordError(err)
		span.SetStatus(codes.Error, "validation failed")
		slog.Error("Streaming request validation failed",
			"error", err,
			"requestId", req.RequestID,
		)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeValidation)
		}
		c.JSON(http.StatusBadRequest, gin.H{"error": "invalid request: validation failed"})
		return
	}

	// Step 3: Scan last user message for policy violations (OUTBOUND protection)
	// This prevents users from sending sensitive data OUT to the LLM
	if len(req.Messages) > 0 {
		lastMsg := req.Messages[len(req.Messages)-1]
		if lastMsg.Role == "user" {
			findings := h.policyEngine.ScanFileContent(lastMsg.Content)
			if len(findings) > 0 {
				span.SetAttributes(attribute.Int("policy.findings_count", len(findings)))
				slog.Warn("Blocked streaming chat: user attempting to send sensitive data",
					"findings_count", len(findings),
					"requestId", req.RequestID,
				)
				if m := observability.DefaultMetrics; m != nil {
					m.RecordError(endpoint, observability.ErrorCodePolicyViolation)
				}
				c.JSON(http.StatusForbidden, gin.H{
					"error":    "Policy Violation: Message contains sensitive data.",
					"findings": findings,
				})
				return
			}
		}
	}

	// Step 4: Set SSE headers and create writer
	SetSSEHeaders(c.Writer)
	sseWriter, err := NewSSEWriter(c.Writer)
	if err != nil {
		span.RecordError(err)
		span.SetStatus(codes.Error, "SSE setup failed")
		slog.Error("Failed to create SSE writer",
			"error", err,
			"requestId", req.RequestID,
		)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeInternal)
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Streaming not supported"})
		return
	}

	// Step 5: Emit status event
	if err := sseWriter.WriteStatus("Generating response..."); err != nil {
		span.RecordError(err)
		slog.Error("Failed to write status event",
			"error", err,
			"requestId", req.RequestID,
		)
		return
	}

	// Step 6: Start heartbeat goroutine to prevent connection timeouts
	heartbeatDone := make(chan struct{})
	go h.runHeartbeat(ctx, sseWriter, endpoint, heartbeatDone)

	// Step 7: Stream tokens from LLM
	// Inbound content (LLM → user) is allowed and logged via hash chain
	params := llm.GenerationParams{
		EnableThinking:  req.EnableThinking,
		BudgetTokens:    req.BudgetTokens,
		ToolDefinitions: req.Tools,
	}

	var tokenCount int32
	firstTokenTime := time.Time{}
	streamErr := h.streamFromLLMWithMetrics(ctx, req.RequestID, req.Messages, params, sseWriter, endpoint, &tokenCount, &firstTokenTime)

	// Stop heartbeat
	close(heartbeatDone)

	if streamErr != nil {
		span.RecordError(streamErr)
		span.SetStatus(codes.Error, "LLM streaming failed")
		span.SetAttributes(attribute.Int("stream.token_count", int(tokenCount)))
		slog.Error("LLM streaming failed",
			"error", streamErr,
			"requestId", req.RequestID,
			"tokenCount", tokenCount,
		)

		// Categorize error for metrics
		if errors.Is(streamErr, context.Canceled) {
			if m := observability.DefaultMetrics; m != nil {
				m.RecordError(endpoint, observability.ErrorCodeClientDisconnect)
				m.RecordClientDisconnect(endpoint)
			}
		} else {
			if m := observability.DefaultMetrics; m != nil {
				m.RecordError(endpoint, observability.ErrorCodeLLMError)
			}
		}
		// Error already sent via SSE
		return
	}

	// Record time to first token
	if !firstTokenTime.IsZero() {
		ttft := firstTokenTime.Sub(startTime).Seconds()
		span.SetAttributes(attribute.Float64("stream.time_to_first_token_seconds", ttft))
		if m := observability.DefaultMetrics; m != nil {
			m.RecordTimeToFirstToken(endpoint, ttft)
		}
	}

	span.SetAttributes(attribute.Int("stream.token_count", int(tokenCount)))

	// Step 8: Emit done event
	if err := sseWriter.WriteDone(req.RequestID); err != nil {
		span.RecordError(err)
		slog.Error("Failed to write done event",
			"error", err,
			"requestId", req.RequestID,
		)
		return
	}

	success = true
	span.SetStatus(codes.Ok, "stream completed successfully")
}

// HandleChatRAGStream processes conversational RAG requests with SSE streaming.
//
// # Description
//
// Handles POST /v1/chat/rag/stream requests. The flow is:
//  1. Parse request body
//  2. Scan user message for policy violations (outbound protection)
//  3. Set SSE headers and create writer
//  4. Emit status event for retrieval
//  5. Retrieve context from RAG service
//  6. Scan and log if retrieved context contains PII (audit trail)
//  7. Emit sources event
//  8. Emit status event for generation
//  9. Stream tokens from LLM via ChatStream
//  10. Emit done event with session ID
//
// # Security
//
//   - Outbound (user → system): Scanned and blocked if contains sensitive data
//   - RAG context (DB → LLM): Scanned and LOGGED if contains PII (audit trail)
//     Future: granite-guardian integration for user acknowledgment flow
//   - Inbound (LLM → user): Allowed, logged via hash chain for async audit
//
// # Inputs
//
//   - c: Gin context containing the HTTP request
//
// Request Body (datatypes.ChatRAGRequest):
//   - message: Required. User's query.
//   - session_id: Optional. Existing session to continue.
//   - pipeline: Optional. RAG pipeline name (default: "reranking").
//   - bearing: Optional. Topic filter for retrieval.
//
// # Outputs
//
// SSE Events:
//   - event: status, data: {"type":"status","message":"Searching knowledge base..."}
//   - event: sources, data: {"type":"sources","sources":[...]}
//   - event: status, data: {"type":"status","message":"Generating response..."}
//   - event: token, data: {"type":"token","content":"..."}
//   - event: done, data: {"type":"done","session_id":"..."}
//   - event: error, data: {"type":"error","error":"..."}
//
// HTTP Status (before streaming starts):
//   - 400 Bad Request: Invalid request body
//   - 403 Forbidden: Policy violation detected (sensitive data in outbound message)
//   - 500 Internal Server Error: RAG service not available or SSE setup failure
//
// # Examples
//
// Request:
//
//	POST /v1/chat/rag/stream
//	Accept: text/event-stream
//	{"message": "What is OAuth?", "pipeline": "reranking"}
//
// Response (SSE stream):
//
//	event: status
//	data: {"type":"status","message":"Searching knowledge base..."}
//
//	event: sources
//	data: {"type":"sources","sources":[{"source":"oauth.md","score":0.95}]}
//
//	event: status
//	data: {"type":"status","message":"Generating response..."}
//
//	event: token
//	data: {"type":"token","content":"OAuth"}
//
//	event: done
//	data: {"type":"done","session_id":"sess-abc123"}
//
// # Limitations
//
//   - Requires ragService to be non-nil
//   - Errors during streaming are sent as events, not HTTP errors
//
// # Assumptions
//
//   - RAG service and Weaviate are available
//   - LLM client supports ChatStream method
//   - Client supports SSE
//
// # Security References
//
//   - SEC-005: Internal errors not exposed to client
func (h *streamingChatHandler) HandleChatRAGStream(c *gin.Context) {
	startTime := time.Now()
	endpoint := observability.EndpointRAGStream

	ctx, span := h.tracer.Start(c.Request.Context(), "HandleChatRAGStream")
	defer span.End()

	// Track active stream (for metrics)
	if m := observability.DefaultMetrics; m != nil {
		m.StreamStarted(endpoint)
		defer m.StreamEnded(endpoint)
	}

	success := false
	defer func() {
		// Record final metrics
		if m := observability.DefaultMetrics; m != nil {
			duration := time.Since(startTime).Seconds()
			m.RecordRequest(endpoint, success)
			m.RecordStreamDuration(endpoint, duration, success)
		}
	}()

	// Check if RAG service is available
	if h.ragService == nil {
		slog.Error("RAG service not configured for streaming")
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeInternal)
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "RAG service not available"})
		return
	}

	// Step 1: Parse request body
	var req datatypes.ChatRAGRequest
	if err := c.BindJSON(&req); err != nil {
		span.RecordError(err)
		span.SetStatus(codes.Error, "invalid request body")
		slog.Error("Failed to parse streaming RAG request", "error", err)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeValidation)
		}
		c.JSON(http.StatusBadRequest, gin.H{"error": "invalid request body"})
		return
	}

	// Add request attributes to span
	span.SetAttributes(
		attribute.String("request.id", req.Id),
		attribute.String("request.pipeline", req.Pipeline),
		attribute.String("request.session_id", req.SessionId),
	)

	// Step 2: Scan user message for policy violations (OUTBOUND protection)
	// This prevents users from sending sensitive data OUT
	findings := h.policyEngine.ScanFileContent(req.Message)
	if len(findings) > 0 {
		span.SetAttributes(attribute.Int("policy.findings_count", len(findings)))
		slog.Warn("Blocked streaming RAG: user attempting to send sensitive data",
			"findings_count", len(findings),
			"requestId", req.Id,
		)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodePolicyViolation)
		}
		c.JSON(http.StatusForbidden, gin.H{
			"error":    "Policy Violation: Message contains sensitive data.",
			"findings": findings,
		})
		return
	}

	// Step 3: Set SSE headers and create writer
	SetSSEHeaders(c.Writer)
	sseWriter, err := NewSSEWriter(c.Writer)
	if err != nil {
		span.RecordError(err)
		span.SetStatus(codes.Error, "SSE setup failed")
		slog.Error("Failed to create SSE writer",
			"error", err,
			"requestId", req.Id,
		)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeInternal)
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Streaming not supported"})
		return
	}

	// Step 4: Emit status event for retrieval
	if err := sseWriter.WriteStatus("Searching knowledge base..."); err != nil {
		span.RecordError(err)
		slog.Error("Failed to write retrieval status event",
			"error", err,
			"requestId", req.Id,
		)
		return
	}

	// Step 5: Start heartbeat for RAG retrieval (can be slow)
	heartbeatDone := make(chan struct{})
	go h.runHeartbeat(ctx, sseWriter, endpoint, heartbeatDone)

	// Step 6: Retrieve context from RAG service
	ragCtx, sources, err := h.retrieveRAGContext(ctx, &req)
	if err != nil {
		close(heartbeatDone)
		span.RecordError(err)
		span.SetStatus(codes.Error, "RAG retrieval failed")
		slog.Error("RAG retrieval failed",
			"error", err,
			"requestId", req.Id,
		)
		if m := observability.DefaultMetrics; m != nil {
			m.RecordError(endpoint, observability.ErrorCodeRAGError)
		}
		_ = sseWriter.WriteError("Failed to retrieve context")
		return
	}

	span.SetAttributes(attribute.Int("rag.sources_count", len(sources)))

	// Step 7: Scan retrieved context for PII (AUDIT logging, not blocking)
	// This logs when database content contains sensitive data being sent to LLM
	// Future: integrate granite-guardian for user acknowledgment flow
	h.auditRAGContextForPII(req.Id, ragCtx, sources)

	// Step 8: Emit sources event
	if err := sseWriter.WriteSources(sources); err != nil {
		close(heartbeatDone)
		span.RecordError(err)
		slog.Error("Failed to write sources event",
			"error", err,
			"requestId", req.Id,
		)
		return
	}

	// Step 9: Emit status event for generation
	if err := sseWriter.WriteStatus("Generating response..."); err != nil {
		close(heartbeatDone)
		span.RecordError(err)
		slog.Error("Failed to write generation status event",
			"error", err,
			"requestId", req.Id,
		)
		return
	}

	// Step 10: Build messages with RAG context and stream
	// Inbound content (LLM → user) is allowed and logged via hash chain
	messages := h.buildRAGMessages(ragCtx, req.Message)
	params := llm.GenerationParams{}

	var tokenCount int32
	firstTokenTime := time.Time{}
	streamErr := h.streamFromLLMWithMetrics(ctx, req.Id, messages, params, sseWriter, endpoint, &tokenCount, &firstTokenTime)

	// Stop heartbeat
	close(heartbeatDone)

	if streamErr != nil {
		span.RecordError(streamErr)
		span.SetStatus(codes.Error, "LLM streaming failed")
		span.SetAttributes(attribute.Int("stream.token_count", int(tokenCount)))
		slog.Error("RAG LLM streaming failed",
			"error", streamErr,
			"requestId", req.Id,
			"tokenCount", tokenCount,
		)

		// Categorize error for metrics
		if errors.Is(streamErr, context.Canceled) {
			if m := observability.DefaultMetrics; m != nil {
				m.RecordError(endpoint, observability.ErrorCodeClientDisconnect)
				m.RecordClientDisconnect(endpoint)
			}
		} else {
			if m := observability.DefaultMetrics; m != nil {
				m.RecordError(endpoint, observability.ErrorCodeLLMError)
			}
		}
		// Error already sent via SSE
		return
	}

	// Record time to first token
	if !firstTokenTime.IsZero() {
		ttft := firstTokenTime.Sub(startTime).Seconds()
		span.SetAttributes(attribute.Float64("stream.time_to_first_token_seconds", ttft))
		if m := observability.DefaultMetrics; m != nil {
			m.RecordTimeToFirstToken(endpoint, ttft)
		}
	}

	span.SetAttributes(attribute.Int("stream.token_count", int(tokenCount)))

	// Step 11: Emit done event with session ID
	sessionID := req.SessionId
	if sessionID == "" {
		sessionID = req.Id
	}
	if err := sseWriter.WriteDone(sessionID); err != nil {
		span.RecordError(err)
		slog.Error("Failed to write done event",
			"error", err,
			"requestId", req.Id,
		)
		return
	}

	success = true
	span.SetStatus(codes.Ok, "stream completed successfully")
}

// =============================================================================
// Helper Methods
// =============================================================================

// runHeartbeat sends periodic keepalive pings to prevent connection timeouts.
//
// # Description
//
// Runs in a separate goroutine, sending SSE comments every heartbeatInterval
// to keep the connection alive during long operations (RAG retrieval, LLM thinking).
// Stops when done channel is closed or context is cancelled.
//
// # Inputs
//
//   - ctx: Context for cancellation detection.
//   - writer: SSE writer to send keepalives.
//   - endpoint: Endpoint name for metrics.
//   - done: Channel to signal when to stop (close to stop).
//
// # Outputs
//
// None. Runs until done is closed or context is cancelled.
//
// # Examples
//
//	done := make(chan struct{})
//	go h.runHeartbeat(ctx, writer, endpoint, done)
//	// ... do work ...
//	close(done)
//
// # Limitations
//
//   - Errors writing keepalives are logged but don't stop the heartbeat.
//
// # Assumptions
//
//   - Writer is thread-safe.
func (h *streamingChatHandler) runHeartbeat(
	ctx context.Context,
	writer SSEWriter,
	endpoint observability.Endpoint,
	done <-chan struct{},
) {
	ticker := time.NewTicker(heartbeatInterval)
	defer ticker.Stop()

	for {
		select {
		case <-done:
			return
		case <-ctx.Done():
			return
		case <-ticker.C:
			if err := writer.WriteKeepAlive(); err != nil {
				slog.Debug("Failed to write keepalive", "error", err)
				return
			}
			if m := observability.DefaultMetrics; m != nil {
				m.RecordKeepAlive(endpoint)
			}
		}
	}
}

// streamFromLLMWithMetrics streams tokens with metrics tracking.
//
// # Description
//
// Enhanced version of streamFromLLM that tracks token count and time to first token.
// Also includes explicit context cancellation checks for cost control.
//
// # Inputs
//
//   - ctx: Context for cancellation.
//   - requestID: Request identifier for logging.
//   - messages: Conversation messages.
//   - params: Generation parameters.
//   - writer: SSE writer for output.
//   - endpoint: Endpoint name for metrics.
//   - tokenCount: Pointer to atomic counter for tokens.
//   - firstTokenTime: Pointer to time of first token (set once).
//
// # Outputs
//
//   - error: Non-nil if streaming failed.
//
// # Security
//
// LLM output is streamed to user and logged via hash chain. This allows
// async review for compliance while not blocking the streaming experience.
//
// # Limitations
//
//   - Requires LLM client to implement ChatStream.
//
// # Assumptions
//
//   - Writer is ready for events.
func (h *streamingChatHandler) streamFromLLMWithMetrics(
	ctx context.Context,
	requestID string,
	messages []datatypes.Message,
	params llm.GenerationParams,
	writer SSEWriter,
	endpoint observability.Endpoint,
	tokenCount *int32,
	firstTokenTime *time.Time,
) error {
	callback := func(event llm.StreamEvent) error {
		// Explicit context cancellation check (cost control)
		// Stop processing immediately if client disconnected
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		switch event.Type {
		case llm.StreamEventToken:
			// Track first token time
			if firstTokenTime.IsZero() {
				*firstTokenTime = time.Now()
			}
			atomic.AddInt32(tokenCount, 1)
			return writer.WriteToken(event.Content)

		case llm.StreamEventThinking:
			return writer.WriteThinking(event.Content)

		case llm.StreamEventError:
			// SEC-005: Sanitize error before sending to client
			sanitizedErr := sanitizeErrorForClient(event.Error)
			return writer.WriteError(sanitizedErr)
		}
		return nil
	}

	err := h.llmClient.ChatStream(ctx, messages, params, callback)
	if err != nil {
		// SEC-005: Log full error internally, send sanitized to client
		slog.Error("LLM ChatStream failed",
			"requestId", requestID,
			"error", err,
			"tokenCount", atomic.LoadInt32(tokenCount),
		)
		_ = writer.WriteError(sanitizeErrorForClient(err.Error()))
		return err
	}

	return nil
}

// streamFromLLM streams tokens from the LLM to the SSE writer (legacy).
//
// # Description
//
// Calls the LLM's ChatStream method and writes tokens to the SSE writer
// as they arrive. Handles thinking tokens separately. All output is
// logged via hash chain for async audit review.
//
// NOTE: Consider using streamFromLLMWithMetrics for new code.
//
// # Inputs
//
//   - ctx: Context for cancellation.
//   - requestID: Request identifier for logging.
//   - messages: Conversation messages.
//   - params: Generation parameters.
//   - writer: SSE writer for output.
//
// # Outputs
//
//   - error: Non-nil if streaming failed.
//
// # Security
//
// LLM output is streamed to user and logged via hash chain. This allows
// async review for compliance while not blocking the streaming experience.
//
// # Limitations
//
//   - Requires LLM client to implement ChatStream.
//
// # Assumptions
//
//   - Writer is ready for events.
func (h *streamingChatHandler) streamFromLLM(
	ctx context.Context,
	requestID string,
	messages []datatypes.Message,
	params llm.GenerationParams,
	writer SSEWriter,
) error {
	callback := func(event llm.StreamEvent) error {
		switch event.Type {
		case llm.StreamEventToken:
			return writer.WriteToken(event.Content)
		case llm.StreamEventThinking:
			return writer.WriteThinking(event.Content)
		case llm.StreamEventError:
			// SEC-005: Sanitize error before sending to client
			sanitizedErr := sanitizeErrorForClient(event.Error)
			return writer.WriteError(sanitizedErr)
		}
		return nil
	}

	err := h.llmClient.ChatStream(ctx, messages, params, callback)
	if err != nil {
		// SEC-005: Log full error internally, send sanitized to client
		slog.Error("LLM ChatStream failed",
			"requestId", requestID,
			"error", err,
		)
		_ = writer.WriteError(sanitizeErrorForClient(err.Error()))
		return err
	}

	return nil
}

// auditRAGContextForPII scans retrieved RAG context and logs PII findings.
//
// # Description
//
// Scans the RAG context string for PII using the policy engine.
// Findings are LOGGED for audit trail but do NOT block the request.
// This provides visibility into sensitive data flowing from DB to LLM.
//
// Future enhancement: Integrate granite-guardian for user acknowledgment
// flow before sending PII-containing context to LLM.
//
// # Inputs
//
//   - requestID: Request identifier for log correlation.
//   - ragContext: The retrieved context text to scan.
//   - sources: Source metadata for logging.
//
// # Outputs
//
// None. Findings are logged only.
//
// # Security
//
// This is an AUDIT function, not a blocking function. The security model
// allows users to see data from the database - we just want visibility
// into what sensitive data is being processed.
//
// # Limitations
//
//   - Does not block requests, only logs.
//   - Future: Add granite-guardian integration.
//
// # Assumptions
//
//   - Policy engine is available.
func (h *streamingChatHandler) auditRAGContextForPII(requestID string, ragContext string, sources []datatypes.SourceInfo) {
	findings := h.policyEngine.ScanFileContent(ragContext)
	if len(findings) > 0 {
		// AUDIT LOG: PII detected in RAG context being sent to LLM
		// This is logged but NOT blocked per security model
		sourceNames := make([]string, 0, len(sources))
		for _, s := range sources {
			sourceNames = append(sourceNames, s.Source)
		}
		slog.Warn("AUDIT: RAG context contains potential PII being sent to LLM",
			"requestId", requestID,
			"sources", sourceNames,
			"findings_count", len(findings),
			"findings_types", extractFindingTypes(findings),
		)
	}
}

// extractFindingTypes extracts the classification names from policy findings for logging.
//
// # Description
//
// Helper to extract finding classification names for structured logging without
// logging the actual sensitive content.
//
// # Inputs
//
//   - findings: Policy scan findings to extract types from.
//
// # Outputs
//
//   - []string: List of finding classification names.
func extractFindingTypes(findings []policy_engine.ScanFinding) []string {
	types := make([]string, 0, len(findings))
	for _, f := range findings {
		types = append(types, f.ClassificationName)
	}
	return types
}

// sanitizeErrorForClient removes internal details from error messages.
//
// # Description
//
// Per SEC-005, internal error details (stack traces, file paths, internal
// service names) must not be exposed to clients. This function returns
// a generic, safe error message.
//
// # Inputs
//
//   - errMsg: Raw error message (may contain internal details).
//
// # Outputs
//
//   - string: Sanitized error message safe for client display.
//
// # Security References
//
//   - SEC-005: Internal errors not exposed to client
func sanitizeErrorForClient(errMsg string) string {
	// Log the full error internally for debugging
	slog.Debug("Sanitizing error for client", "original_error", errMsg)

	// Return generic message - don't expose internals
	return "An error occurred while processing your request"
}

// retrieveRAGContext retrieves context from the RAG service.
//
// # Description
//
// Calls the RAG service to retrieve relevant documents for the query.
// Returns the context string and source information for display.
//
// NOTE: Currently the RAG service does full retrieval + LLM generation.
// For true streaming, we need a retrieval-only endpoint that returns
// the actual document content. This is a TODO for future enhancement.
//
// Current workaround: Use the RAG service's answer as context for
// streaming. This means RAG streaming currently makes two LLM calls
// (one in RAG service, one for streaming). Future optimization should
// add a retrieval-only mode to the Python RAG engine.
//
// # Inputs
//
//   - ctx: Context for cancellation.
//   - req: RAG request with message and pipeline info.
//
// # Outputs
//
//   - string: Retrieved context to include in LLM prompt.
//   - []datatypes.SourceInfo: Source documents with scores.
//   - error: Non-nil if retrieval failed.
//
// # Limitations
//
//   - Depends on RAG service availability.
//   - Currently makes two LLM calls (RAG + streaming). TODO: Add retrieval-only mode.
//
// # Assumptions
//
//   - RAG service is properly configured.
func (h *streamingChatHandler) retrieveRAGContext(
	ctx context.Context,
	req *datatypes.ChatRAGRequest,
) (string, []datatypes.SourceInfo, error) {
	// TODO: Implement retrieval-only mode in Python RAG engine
	// Currently the RAG service does retrieval + LLM generation together.
	// For true streaming, we need just the retrieval results (document content).
	//
	// Workaround: Call the full RAG service and use its answer as context.
	// This is inefficient (two LLM calls) but allows streaming to work.
	resp, err := h.ragService.Process(ctx, req)
	if err != nil {
		return "", nil, err
	}

	// Use the RAG answer as context for the streaming LLM call
	// TODO: Replace with actual retrieved document content when
	// retrieval-only mode is implemented
	contextStr := resp.Answer

	return contextStr, resp.Sources, nil
}

// buildRAGMessages constructs messages with RAG context for LLM.
//
// # Description
//
// Builds the message array with system prompt containing retrieved context
// and the user's question.
//
// # Inputs
//
//   - ragContext: Retrieved document context.
//   - userMessage: User's original question.
//
// # Outputs
//
//   - []datatypes.Message: Messages ready for LLM.
//
// # Limitations
//
//   - System prompt is hardcoded.
//
// # Assumptions
//
//   - Context is already formatted.
func (h *streamingChatHandler) buildRAGMessages(ragContext, userMessage string) []datatypes.Message {
	systemPrompt := `You are a helpful assistant. Use the following context to answer the user's question.
If the context doesn't contain relevant information, say so and provide what help you can.

Context:
` + ragContext

	return []datatypes.Message{
		{Role: "system", Content: systemPrompt},
		{Role: "user", Content: userMessage},
	}
}

// =============================================================================
// Compile-time Interface Check
// =============================================================================

var _ StreamingChatHandler = (*streamingChatHandler)(nil)
